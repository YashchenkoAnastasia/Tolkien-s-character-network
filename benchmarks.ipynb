{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "from pathlib import Path\n",
    "from pyspark import SparkContext\n",
    "from afinn import Afinn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import en_core_web_sm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_file = open(r'C:\\Users\\NYashch\\nyashch-projects\\all_sentences.txt','r',encoding = 'utf-8')\n",
    "all_sentences = [line.strip() for line in sent_file]\n",
    "sent_file.close()\n",
    "tolkien_text = ''.join(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolkien_text=tolkien_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add common words and races\n",
    "cm_file = open(r'C:\\Users\\NYashch\\nyashch-projects\\Diploma\\common_words.txt', 'r')\n",
    "cm_lines = [line.strip() for line in cm_file]\n",
    "cm_file.close()\n",
    "common_words = []\n",
    "for word in cm_lines:\n",
    "    all_cm = word.split(',')\n",
    "    for cm in all_cm:\n",
    "        cm = cm.replace('\"','')\n",
    "        common_words.append(cm)\n",
    "races = ['trolls','troll', 'werewolves','werewolf', 'half_elven','elf', 'ents','ent', 'eagles','eagle','dragons','dragon', 'barlogs','barlog','ainur','hobbit','hobbits','dwarves','dwarf', 'elves', 'maiar', 'orcs', 'orc', 'valar','valars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_entity_recognition(sentence,names_list):\n",
    "    '''\n",
    "    A function to retrieve name entities in a sentence.\n",
    "    :param sentence: the sentence to retrieve names from.\n",
    "    :return: a name entity list of the sentence.\n",
    "    '''\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    # retrieve person and organization's name from the sentence\n",
    "    name_entity = [x for x in doc.ents if x.label_ in ['PERSON']]\n",
    "    # convert all names to lowercase and remove 's in names\n",
    "    name_entity = [str(x).lower().replace(\"'s\",\"\") for x in name_entity]\n",
    "    # split names into single words ('Harry Potter' -> ['Harry', 'Potter'])\n",
    "    name_entity = [x.split(' ') for x in name_entity]\n",
    "    # flatten the name list\n",
    "    name_entity = flatten(name_entity)\n",
    "    # remove name words that are less than 3 letters to raise recognition accuracy\n",
    "    name_entity = [x for x in name_entity if len(x) >= 3]\n",
    "    # remove name words that are in the set of 4000 common words\n",
    "    name_entity = [x for x in name_entity if x not in common_words]\n",
    "    # remove name words that are in the set of races\n",
    "    name_entity = [x for x in name_entity if x not in races]\n",
    "    #print(name_entity)\n",
    "    for name in name_entity:\n",
    "        names_list.append(name)\n",
    "    return name_entity\n",
    "def flatten(input_list):\n",
    "    '''\n",
    "    A function to flatten complex list.\n",
    "    :param input_list: The list to be flatten\n",
    "    :return: the flattened list.\n",
    "    '''\n",
    "\n",
    "    flat_list = []\n",
    "    for i in input_list:\n",
    "        if type(i) == list:\n",
    "            flat_list += flatten(i)\n",
    "        else:\n",
    "            flat_list += [i]\n",
    "\n",
    "    return flat_list\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 914.1199584007263 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "spacy_ner = []\n",
    "for sent in all_sentences:\n",
    "    name_entity_recognition(sent,spacy_ner)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 238.5028657913208 seconds ---\n"
     ]
    }
   ],
   "source": [
    "nltk_iobs = []\n",
    "start_time = time.time()\n",
    "for sent in all_sentences:\n",
    "    iob = preprocess(sent)\n",
    "    for i in iob:\n",
    "        if list(i)[1] =='NNP':\n",
    "            nltk_iobs.append(list(i)[0])\n",
    "            #print((list(i)[0]))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Golden standart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hobbits = open(r'C:\\Users\\NYashch\\nyashch-projects\\Diploma\\my_hobbits.txt','r',encoding = 'utf-8')\n",
    "hobbits = []\n",
    "for elem in my_hobbits.readlines():\n",
    "    if len(elem) > 4:\n",
    "        elem = elem.replace(\"\\n\",\"\")\n",
    "        elem = elem.replace(\"\\r\",\"\")\n",
    "        hobbits.append(elem.lower().split()[0])\n",
    "\n",
    "d_hobbits = dict.fromkeys(hobbits, 'hobbit')\n",
    "\n",
    "my_dwarves = open(r'C:\\Users\\NYashch\\nyashch-projects\\Diploma\\my_dwarves.txt','r',encoding = 'utf-8')\n",
    "dwarves = []\n",
    "\n",
    "for elem in my_dwarves.readlines():\n",
    "    if len(elem) > 4:\n",
    "        elem = elem.replace(\"\\n\",\"\")\n",
    "        elem = elem.replace(\"\\r\",\"\")\n",
    "        dwarves.append(elem.lower().split()[0])\n",
    "d_dwarves = dict.fromkeys(dwarves, 'dwarf')\n",
    "my_elves = open(r'C:\\Users\\NYashch\\nyashch-projects\\Diploma\\my_elves.txt','r',encoding = 'utf-8')\n",
    "elves = []\n",
    "for elem in my_elves .readlines():\n",
    "    if len(elem) > 4:\n",
    "        elem = elem.replace(\"\\n\",\"\")\n",
    "        elem = elem.replace(\"\\r\",\"\")\n",
    "        elves.append(elem.lower().split()[0])\n",
    "d_elves = dict.fromkeys(elves, 'elf')\n",
    "my_maiar = open(r'C:\\Users\\NYashch\\nyashch-projects\\Diploma\\my_maiar.txt','r',encoding = 'utf-8')\n",
    "maiar = []\n",
    "for elem in my_maiar.readlines():\n",
    "    if len(elem) > 4:\n",
    "        elem = elem.replace(\"\\n\",\"\")\n",
    "        elem = elem.replace(\"\\r\",\"\")\n",
    "        maiar.append(elem.lower().split()[0])\n",
    "d_maiar = dict.fromkeys(maiar, 'maiar')\n",
    "my_orcs = open(r'C:\\Users\\NYashch\\nyashch-projects\\Diploma\\my_orcs.txt','r',encoding = 'utf-8')\n",
    "orcs = []\n",
    "for elem in my_orcs.readlines():\n",
    "    if len(elem) > 4:\n",
    "        elem = elem.replace(\"\\n\",\"\")\n",
    "        elem = elem.replace(\"\\r\",\"\")\n",
    "        orcs.append(elem.lower().split()[0])\n",
    "d_orcs = dict.fromkeys(orcs, 'orc')\n",
    "my_men = open(r'C:\\Users\\NYashch\\nyashch-projects\\Diploma\\my_men.txt','r',encoding = 'utf-8')\n",
    "men = []\n",
    "for elem in my_men.readlines():\n",
    "    if len(elem) > 4:\n",
    "        elem = elem.replace(\"\\n\",\"\")\n",
    "        elem = elem.replace(\"\\r\",\"\")\n",
    "        men.append(elem.lower().split()[0])\n",
    "d_men = dict.fromkeys(men, 'men')\n",
    "my_valars = open(r'C:\\Users\\NYashch\\nyashch-projects\\Diploma\\my_valars.txt','r',encoding = 'utf-8')\n",
    "valars = []\n",
    "for elem in my_valars.readlines():\n",
    "    if len(elem) > 4:\n",
    "        elem = elem.replace(\"\\n\",\"\")\n",
    "        elem = elem.replace(\"\\r\",\"\")\n",
    "        valars.append(elem.lower())\n",
    "d_valars = dict.fromkeys(valars, 'valar')\n",
    "ainur = [\"ainur\",'aluin','danuin','fanuin','morgoth','ranuin']\n",
    "d_ainur = dict.fromkeys(ainur, 'ainur')\n",
    "barlogs = ['gothmog','lungorthin','osombauko']\n",
    "d_barlogs = dict.fromkeys(barlogs, 'barlog')\n",
    "dragons = ['ancalagon','chrysophylax','glaurung','gostir','scatha','smaug','urulókë']\n",
    "d_dragons = dict.fromkeys(dragons, 'dragon')\n",
    "eagles = ['gwaihir','landroval','meneldor','thornhoth','thorondor','windlord']\n",
    "d_eagles = dict.fromkeys(eagles, 'eagle')\n",
    "ents = ['beechbone','entings','entmaidens','entmoot','entwives','fimbrethil','leaflock','onodrim','quickbeam','skinbark','treebeard']\n",
    "d_ents = dict.fromkeys(ents, 'ent')\n",
    "half_elven = ['ardamir eärendil','arwen','dior','eärendil','elladan','elrohir','elrond','elros','elurín','elwing','undómiel']\n",
    "d_half_elven = dict.fromkeys(half_elven, 'half_elf')\n",
    "werewolves = ['carcharoth','draugluin','thû']\n",
    "d_werewolves = dict.fromkeys(werewolves, 'werewolf')\n",
    "trolls = ['bert','olog-hai','tom','william']\n",
    "d_trolls = dict.fromkeys(trolls, 'troll')\n",
    "all_names = hobbits + dwarves +elves+maiar+orcs+men+valars+ainur+barlogs+dragons+eagles+ents+half_elven+werewolves+trolls\n",
    "names = list(set(all_names))\n",
    "names.append('sam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK precision score is 0.062557497700092\n",
      "NLTK recall score is 0.3938223938223938\n",
      "Spacy precision score is 0.09941520467836257\n",
      "Spacy recall score is 0.22972972972972974\n"
     ]
    }
   ],
   "source": [
    "nltk_iobs = set(nltk_iobs)\n",
    "#with NLTK\n",
    "tp_nltk = 0\n",
    "fp_nltk = 0\n",
    "for iob in nltk_iobs:\n",
    "    if iob.lower() in names:\n",
    "        tp_nltk = tp_nltk +1\n",
    "    else:\n",
    "        fp_nltk = fp_nltk+ 1\n",
    "fn_nltk = len(names) - tp_nltk\n",
    "print('NLTK precision score is',tp_nltk / (tp_nltk + fp_nltk))\n",
    "print('NLTK recall score is',tp_nltk / (tp_nltk + fn_nltk))\n",
    "#with Spacy\n",
    "spacy_ner = set(spacy_ner)\n",
    "tp_spacy = 0\n",
    "fp_spacy = 0\n",
    "for ner in spacy_ner:\n",
    "    if ner.lower() in names:\n",
    "        tp_spacy = tp_spacy +1\n",
    "    else:\n",
    "        fp_spacy = fp_spacy+ 1\n",
    "fn_spacy = len(names) - tp_spacy\n",
    "print('Spacy precision score is',tp_spacy / (tp_spacy + fp_spacy))\n",
    "print('Spacy recall score is',tp_spacy / (tp_spacy + fn_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
